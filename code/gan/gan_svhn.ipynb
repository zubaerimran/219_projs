{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan_svhn.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zubaerimran/219_projs/blob/master/code/gan/gan_svhn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "ZNvSHSToV5kL",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "Semi-Supervised Learning GAN - MNIST dataset"
      ]
    },
    {
      "metadata": {
        "id": "JIXsCScNzJOV",
        "colab_type": "code",
        "outputId": "4adf51a8-16fa-4c47-a7e8-3dd22ea53855",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZKL-k3mkV5kU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import pdb\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.contrib.layers.python.layers import batch_norm\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "#%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qG23NyOV5kd",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "Initialization"
      ]
    },
    {
      "metadata": {
        "id": "gKfWmBuhV5ki",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x_height, x_width = [32, 32]\n",
        "num_channels = 3\n",
        "num_classes = 10\n",
        "latent_size = 100\n",
        "labeled_rate = 0.1\n",
        "g_scale_factor = 1 - 0.75/2\n",
        "d_scale_factor = 0.25\n",
        "eps_min = 1e-8 # used to avoid NAN loss\n",
        "eps_max = 1.0\n",
        "alpha = 0.2\n",
        "\n",
        "save_to = 'gdrive/My Drive/gan/svhn/model_Jan16/out/'\n",
        "\n",
        "if not os.path.exists(save_to):\n",
        "    os.makedirs(save_to)\n",
        "\n",
        "log_path = 'gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_log.csv'\n",
        "model_path ='gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P5OJCVO6R1Wx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# norm_tr1 = (norm_tr + 1.0)/2.0\n",
        "# np.max(norm_tr), np.max(norm_tr1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VEOI0-IdQLDB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #checking training data\n",
        "# tr = dataset['train_images']\n",
        "\n",
        "\n",
        "# norm_tr = normalize(tr[:256])\n",
        "# plot(norm_tr1[:16])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oXp_QjpO8FIj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def normalize(x):\n",
        "    # normalize data\n",
        "    x = (x - 127.5) / 127.5\n",
        "    return x.reshape((-1, x_height, x_width, num_channels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DBEKQ4Cv1Vve",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import scipy.io as sio\n",
        "dataset = sio.loadmat('gdrive/My Drive/aelan_datasetes/svhn.mat')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KXN7hxDMHFiX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# plot_fake_data(norm_tr1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VaG5EdwK44cs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #checking training data\n",
        "# tr = dataset['train_images']\n",
        "# plot(tr[:64])\n",
        "\n",
        "# norm_tr = normalize(tr)\n",
        "# norm_tr1 = (norm_tr +1)/2\n",
        "# plot(norm_tr1[:64])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uNg1uf17qmSt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def G(inputs, is_training, reuse = False, print_summary = False):\n",
        "#     with tf.variable_scope('Generator', reuse = reuse) as scope:\n",
        "\n",
        "#         normal_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        \n",
        "#         # Expand input to [batch, 1, 1, channels]\n",
        "#         inputs = tf.expand_dims(tf.expand_dims(inputs, 1), 1)\n",
        "        \n",
        "#         # Transposed convolution outputs [batch, 4, 4, 1024]\n",
        "#         net = tf.layers.conv2d_transpose(inputs, \n",
        "#                                          filters=1024, \n",
        "#                                          kernel_size=4, \n",
        "#                                          padding='valid',\n",
        "#                                          kernel_initializer=normal_initializer,\n",
        "#                                          trainable=is_training,\n",
        "#                                          name='tconv1')\n",
        "\n",
        "#         net = tf.layers.batch_normalization(net, \n",
        "#                                             training=is_training,\n",
        "#                                             name='tconv1/batch_normalization')\n",
        "\n",
        "#         net = tf.nn.relu(net, name='tconv1/relu')\n",
        "        \n",
        "#         # Transposed convolution outputs [batch, 8, 8, 256]\n",
        "#         net = tf.layers.conv2d_transpose(net, \n",
        "#                                          filters=256, \n",
        "#                                          kernel_size=4, \n",
        "#                                          strides=2, \n",
        "#                                          padding='same',\n",
        "#                                          kernel_initializer=normal_initializer,\n",
        "#                                          trainable=is_training,\n",
        "#                                          name='tconv2')\n",
        "\n",
        "#         net = tf.layers.batch_normalization(net, \n",
        "#                                             training=is_training,\n",
        "#                                             name='tconv2/batch_normalization')\n",
        "#         net = tf.nn.relu(net, name='tconv2/relu')\n",
        "        \n",
        "#         # Transposed convolution outputs [batch, 16, 16, 64]\n",
        "#         net = tf.layers.conv2d_transpose(net, \n",
        "#                                          filters=64, \n",
        "#                                          kernel_size=4, \n",
        "#                                          strides=2, \n",
        "#                                          padding='same',\n",
        "#                                          kernel_initializer=normal_initializer,\n",
        "#                                          trainable=is_training,\n",
        "#                                          name='tconv3')\n",
        "\n",
        "#         net = tf.layers.batch_normalization(net, \n",
        "#                                             training=is_training,\n",
        "#                                             name='tconv3/batch_normalization')\n",
        "#         net = tf.nn.relu(net, name='tconv3/relu')\n",
        "        \n",
        "#         # Transposed convolution outputs [batch, 32, 32, 3]\n",
        "#         net = tf.layers.conv2d_transpose(net, \n",
        "#                                          filters=3, \n",
        "#                                          kernel_size=4, \n",
        "#                                          strides=2, \n",
        "#                                          padding='same',\n",
        "#                                          kernel_initializer=normal_initializer,\n",
        "#                                          trainable=is_training,\n",
        "#                                          name='tconv4')\n",
        "\n",
        "#         net = tf.tanh(net, name='tconv4/tanh')\n",
        "        \n",
        "#         return net\n",
        "\n",
        "# def D(inputs, dropout_rate, is_training, reuse = True, print_summary = True):\n",
        "\n",
        "#   with tf.variable_scope('Discriminator', reuse = reuse) as scope:      \n",
        "\n",
        "#         normal_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        \n",
        "#         # Convolution outputs [batch, 16, 16, 64]\n",
        "#         net = tf.layers.conv2d(inputs, \n",
        "#                                filters=64, \n",
        "#                                kernel_size=4, \n",
        "#                                strides=2, \n",
        "#                                padding='same',\n",
        "#                                kernel_initializer=normal_initializer,\n",
        "#                                trainable=is_training,\n",
        "#                                name='conv1')\n",
        "\n",
        "#         net = leaky_relu(net, 0.2, name='conv1/leaky_relu')\n",
        "        \n",
        "#         # Convolution outputs [batch, 8, 8, 256]\n",
        "#         net = tf.layers.conv2d(net, \n",
        "#                                filters=256, \n",
        "#                                kernel_size=4, \n",
        "#                                strides=2, \n",
        "#                                padding='same',\n",
        "#                                kernel_initializer=normal_initializer,\n",
        "#                                trainable=is_training,\n",
        "#                                name='conv2')\n",
        "\n",
        "#         net = tf.layers.batch_normalization(net, \n",
        "#                                             training=is_training,\n",
        "#                                             name='conv2/batch_normalization')\n",
        "\n",
        "#         net = leaky_relu(net, 0.2, name='conv2/leaky_relu')\n",
        "        \n",
        "#         # Convolution outputs [batch, 4, 4, 1024]\n",
        "#         net = tf.layers.conv2d(net, \n",
        "#                                filters=1024, \n",
        "#                                kernel_size=4, \n",
        "#                                strides=2, \n",
        "#                                padding='same',\n",
        "#                                kernel_initializer=normal_initializer,\n",
        "#                                trainable=is_training,\n",
        "#                                name='conv3')\n",
        "\n",
        "#         net = tf.layers.batch_normalization(net, \n",
        "#                                             training=is_training,\n",
        "#                                             name='conv3/batch_normalization')\n",
        "\n",
        "#         net = leaky_relu(net, 0.2, name='conv3/leaky_relu')\n",
        "        \n",
        "#         # Convolution outputs [batch, 1, 1, 1]\n",
        "# #         net = tf.layers.conv2d(net, \n",
        "# #                                filters=1, \n",
        "# #                                kernel_size=4, \n",
        "# #                                padding='valid',\n",
        "# #                                kernel_initializer=normal_initializer,\n",
        "# #                                trainable=is_training,\n",
        "# #                                name='conv4')\n",
        "        \n",
        "#         # Squeeze height and width dimensions\n",
        "#         #net = tf.squeeze(net, [1, 2, 3])\n",
        "        \n",
        "#         flatten_length = lrelu4.get_shape().as_list()[1] * \\\n",
        "# #                          lrelu4.get_shape().as_list()[2] * lrelu4.get_shape().as_list()[3]\n",
        "# #         flatten5 = tf.reshape(lrelu4, (-1, flatten_length)) # used for \"Feature Matching\" \n",
        "# #         fc5 = tf.layers.dense(flatten5, (num_classes + 1))\n",
        "# #         output = tf.nn.softmax(fc5)\n",
        "\n",
        "# #         assert output.get_shape()[1:] == [num_classes + 1]\n",
        "\n",
        "# #         if print_summary:\n",
        "# #             print('Discriminator summary:\\n x: %s\\n' \\\n",
        "# #                   ' D1: %s\\n D2: %s\\n D3: %s\\n D4: %s\\n' %(x.get_shape(), \n",
        "# #                                                            dropout1.get_shape(),\n",
        "# #                                                            lrelu2.get_shape(), \n",
        "# #                                                            dropout3.get_shape(),\n",
        "# #                                                            lrelu4.get_shape()))\n",
        "# #         return flatten5, fc5, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "0r4BhQggV5lI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def D(x, dropout_rate, is_training, reuse = True, print_summary = True):\n",
        "    # discriminator (x -> n + 1 class)\n",
        "\n",
        "    with tf.variable_scope('Discriminator', reuse = reuse) as scope:\n",
        "        normal_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        # layer1 - do not use Batch Normalization on the first layer of Discriminator\n",
        "        conv1 = tf.layers.conv2d(x, 64, [5, 5],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same',\n",
        "                                 kernel_initializer=normal_initializer,)\n",
        "        lrelu1 = tf.maximum(0.2 * conv1, conv1) #leaky relu\n",
        "        dropout1 = tf.layers.dropout(lrelu1, dropout_rate)\n",
        "\n",
        "        # layer2\n",
        "        conv2 = tf.layers.conv2d(dropout1, 128, [3, 3],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same',\n",
        "                                 kernel_initializer=normal_initializer)\n",
        "        batch_norm2 = tf.layers.batch_normalization(conv2, training = is_training)\n",
        "        lrelu2 = tf.maximum(0.2 * batch_norm2, batch_norm2)\n",
        "\n",
        "        # layer3\n",
        "        conv3 = tf.layers.conv2d(lrelu2, 256, [2, 2],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same',\n",
        "                                 kernel_initializer=normal_initializer)\n",
        "        batch_norm3 = tf.layers.batch_normalization(conv3, training = is_training)\n",
        "        lrelu3 = tf.maximum(0.2 * batch_norm3, batch_norm3)\n",
        "        dropout3 = tf.layers.dropout(lrelu3, dropout_rate)\n",
        "\n",
        "        # layer 4\n",
        "        conv4 = tf.layers.conv2d(dropout3, 512, [2, 2],\n",
        "                                 strides = [2, 2],\n",
        "                                 padding = 'same',\n",
        "                                 kernel_initializer=normal_initializer)\n",
        "        # do not use batch_normalization on this layer - next layer, \"flatten5\",\n",
        "        # will be used for \"Feature Matching\"\n",
        "        lrelu4 = tf.maximum(0.2 * conv4, conv4)\n",
        "\n",
        "        # layer 5\n",
        "        flatten_length = lrelu4.get_shape().as_list()[1] * \\\n",
        "                         lrelu4.get_shape().as_list()[2] * lrelu4.get_shape().as_list()[3]\n",
        "        flatten5 = tf.reshape(lrelu4, (-1, flatten_length)) # used for \"Feature Matching\" \n",
        "        fc5 = tf.layers.dense(flatten5, (num_classes + 1))\n",
        "        output = tf.nn.softmax(fc5)\n",
        "\n",
        "        assert output.get_shape()[1:] == [num_classes + 1]\n",
        "\n",
        "        if print_summary:\n",
        "            print('Discriminator summary:\\n x: %s\\n' \\\n",
        "                  ' D1: %s\\n D2: %s\\n D3: %s\\n D4: %s\\n' %(x.get_shape(), \n",
        "                                                           dropout1.get_shape(),\n",
        "                                                           lrelu2.get_shape(), \n",
        "                                                           dropout3.get_shape(),\n",
        "                                                           lrelu4.get_shape()))\n",
        "        return flatten5, fc5, output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tepkkLAKV5lQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "alpha = 0.2\n",
        "def G(z, is_training, reuse = False, print_summary = False):\n",
        "    # generator (z -> x)\n",
        "\n",
        "    with tf.variable_scope('Generator', reuse = reuse) as scope:\n",
        "        normal_initializer = tf.random_normal_initializer(mean=0.0, stddev=0.02)\n",
        "        # layer 0\n",
        "        z_ = tf.layers.dense(z, 512*2*2)\n",
        "        z_reshaped = tf.reshape(z_, (-1, 2, 2, 512))\n",
        "        z_dense = tf.nn.relu(z_reshaped)\n",
        "\n",
        "        # layer 1 out: 8x8\n",
        "        deconv1 = tf.layers.conv2d_transpose(z_dense,\n",
        "                                             filters = 256,\n",
        "                                             kernel_size = [5, 5],\n",
        "                                             strides = [2, 2],\n",
        "                                             padding = 'same',\n",
        "                                             kernel_initializer=normal_initializer)\n",
        "        batch_norm1 = tf.layers.batch_normalization(deconv1, training = is_training)\n",
        "        relu1 = tf.nn.relu(batch_norm1)\n",
        "        \n",
        "        \n",
        "        \n",
        "        # layer 2 out: 16x16\n",
        "        deconv2 = tf.layers.conv2d_transpose(relu1,\n",
        "                                             filters = 128,\n",
        "                                             kernel_size = [5, 5],\n",
        "                                             strides = [2, 2],\n",
        "                                             padding = 'same',\n",
        "                                             kernel_initializer=normal_initializer)\n",
        "        batch_norm2 = tf.layers.batch_normalization(deconv2, training = is_training)\n",
        "        relu2 = tf.nn.relu(batch_norm2)\n",
        "        \n",
        "        \n",
        "        # layer 3 out:32x32\n",
        "        deconv3 = tf.layers.conv2d_transpose(relu2,\n",
        "                                             filters = 64,\n",
        "                                             kernel_size = [5, 5],\n",
        "                                             strides = [2, 2],\n",
        "                                             padding = 'same',\n",
        "                                             kernel_initializer=normal_initializer)\n",
        "        batch_norm3 = tf.layers.batch_normalization(deconv3, training = is_training)\n",
        "        relu3 = tf.nn.relu(batch_norm3)\n",
        "\n",
        "        \n",
        "#         # layer 4 out: 64x64\n",
        "#         deconv4 = tf.layers.conv2d_transpose(lrelu3,\n",
        "#                                              filters = 64,\n",
        "#                                              kernel_size = [3, 3],\n",
        "#                                              strides = [2, 2],\n",
        "#                                              padding = 'same')\n",
        "#         batch_norm4 = tf.layers.batch_normalization(deconv4, training = is_training)\n",
        "#         lrelu4 = tf.nn.leaky_relu(batch_norm4, alpha)\n",
        "        \n",
        "        \n",
        "#         # layer 5 out: 128x128\n",
        "#         deconv5 = tf.layers.conv2d_transpose(lrelu4,\n",
        "#                                              filters = 32,\n",
        "#                                              kernel_size = [3, 3],\n",
        "#                                              strides = [2, 2],\n",
        "#                                              padding = 'same')\n",
        "#         batch_norm5 = tf.layers.batch_normalization(deconv4, training = is_training)\n",
        "#         lrelu5 = tf.nn.leaky_relu(batch_norm5, alpha)\n",
        "        \n",
        "        \n",
        "        # layer 5 - do not use Batch Normalization on the last layer of Generator\n",
        "        deconv4 = tf.layers.conv2d_transpose(relu3,\n",
        "                                             filters = num_channels,\n",
        "                                             kernel_size = [3, 3],\n",
        "                                             strides = [2, 2],\n",
        "                                             padding = 'same',\n",
        "                                             kernel_initializer=normal_initializer)\n",
        "        tanh4 = tf.tanh(deconv4)\n",
        "        \n",
        "        print(tanh4.shape)\n",
        "                                             \n",
        "                                             \n",
        "        assert tanh4.get_shape()[1:] == [x_height, x_width, num_channels]\n",
        "        \n",
        "        if print_summary:\n",
        "            print('Generator summary:\\n z: %s\\n' \\\n",
        "                  ' G0: %s\\n G1: %s\\n G2: %s\\n G3: %s\\n G4: %s\\n' %(z_.get_shape(),\n",
        "                                                                    z_reshaped.get_shape(),\n",
        "                                                                    relu1.get_shape(),\n",
        "                                                                    relu2.get_shape(),\n",
        "                                                                    relu3.get_shape(),\n",
        "                                                                    #lrelu4.get_shape(),\n",
        "                                                                    tanh4.get_shape()))\n",
        "        return tanh4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eYm1ntQ4V5ld",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#For single D\n",
        "def build_model(x_real, z, label, dropout_rate, is_training, print_summary = False):\n",
        "    # build model\n",
        "    D_real_features, D_real_logit, D_real_prob = D(x_real, dropout_rate, is_training,\n",
        "                                                   reuse = False, print_summary = print_summary)\n",
        "    x_fake = G(z, is_training, reuse = False, print_summary = print_summary)\n",
        "    D_fake_features, D_fake_logit, D_fake_prob = D(x_fake, dropout_rate, is_training,\n",
        "                                                   reuse = True, print_summary = print_summary)\n",
        "\n",
        "    return D_real_features, D_real_logit, D_real_prob, D_fake_features, D_fake_logit, D_fake_prob, x_fake"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "krc7CIr1BCjQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def clip_value(value):\n",
        "  value = tf.clip_by_value(value, eps_min, eps_max)\n",
        "  return value"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wHWHvfcZV5lw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def prepare_labels(label):\n",
        "    # add extra label for fake data\n",
        "    extended_label = tf.concat([label, tf.zeros([tf.shape(label)[0], 1])], axis = 1)\n",
        "\n",
        "    return extended_label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mJtlyPUoV5mA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def loss_accuracy(D_real_features, D_real_logit, D_real_prob, D_fake_features,\n",
        "                  D_fake_logit, D_fake_prob, extended_label, labeled_mask):\n",
        "    epsilon = 1e-8 # used to avoid NAN loss\n",
        "    # *** Discriminator loss ***\n",
        "    # supervised loss\n",
        "    # which class the real data belongs to\n",
        "    tmp = tf.nn.softmax_cross_entropy_with_logits(logits = D_real_logit,\n",
        "                                                  labels = extended_label)\n",
        "    D_L_supervised = tf.reduce_sum(labeled_mask * tmp) / tf.reduce_sum(labeled_mask) # to ignore\n",
        "                                                                                     # unlabeled\n",
        "                                                                                     # data\n",
        "\n",
        "    # unsupervised loss\n",
        "    # data is real\n",
        "    prob_real_be_real = 1 - D_real_prob[:, -1] + epsilon\n",
        "    tmp_log = tf.log(prob_real_be_real)\n",
        "    D_L_unsupervised1 = -1 * tf.reduce_mean(tmp_log)\n",
        "\n",
        "    # data is fake\n",
        "    prob_fake_be_fake = D_fake_prob[:, -1] + epsilon\n",
        "    tmp_log = tf.log(prob_fake_be_fake)\n",
        "    D_L_unsupervised2 = -1 * tf.reduce_mean(tmp_log)\n",
        "\n",
        "    D_L = D_L_supervised + D_L_unsupervised1 + D_L_unsupervised2\n",
        "\n",
        "    # *** Generator loss ***\n",
        "    # fake data is mistaken to be real\n",
        "    prob_fake_be_real = 1 - D_fake_prob[:, -1] + epsilon\n",
        "    tmp_log =  tf.log(prob_fake_be_real)\n",
        "    G_L1 = -1 * tf.reduce_mean(tmp_log)\n",
        "\n",
        "    # Feature Maching\n",
        "    tmp1 = tf.reduce_mean(D_real_features, axis = 0)\n",
        "    tmp2 = tf.reduce_mean(D_fake_features, axis = 0)\n",
        "    G_L2 = tf.reduce_mean(tf.square(tmp1 - tmp2))\n",
        "\n",
        "    G_L = G_L1 + G_L2\n",
        "\n",
        "    # accuracy\n",
        "    correct_prediction = tf.equal(tf.argmax(D_real_prob[:, :-1], 1),\n",
        "                                  tf.argmax(extended_label[:, :-1], 1))\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    return D_L_supervised, D_L_unsupervised1, D_L_unsupervised2, D_L, G_L, accuracy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Onxp_VKUV5mM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def optimizer(D_Loss, G_Loss, D_learning_rate, G_learning_rate):\n",
        "    # D and G optimizer\n",
        "    extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "    \n",
        "    with tf.control_dependencies(extra_update_ops):\n",
        "        all_vars = tf.trainable_variables()\n",
        "        D_vars = [var for var in all_vars if var.name.startswith('Discriminator')]\n",
        "        G_vars = [var for var in all_vars if var.name.startswith('Generator')]\n",
        "\n",
        "        D_optimizer = tf.train.AdamOptimizer(D_learning_rate).minimize(D_Loss, var_list = D_vars)\n",
        "        G_optimizer = tf.train.AdamOptimizer(G_learning_rate).minimize(G_Loss, var_list = G_vars)\n",
        "\n",
        "        return D_optimizer, G_optimizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cGkh-l2GV5mT",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "Visualize"
      ]
    },
    {
      "metadata": {
        "id": "IQSTGipyV5mc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def plot_fake_data(data, grid_size = [5, 5]):\n",
        "    # visualize some data generated by G\n",
        "    _, axes = plt.subplots(figsize = grid_size, nrows = grid_size[0], ncols = grid_size[1],\n",
        "                           sharey = True, sharex = True)\n",
        "\n",
        "    size = grid_size[0] * grid_size[1]\n",
        "    index = np.int_(np.random.uniform(0, data.shape[0], size = (size)))\n",
        "    figs = data[index].reshape(-1, x_height, x_width, num_channels)\n",
        "\n",
        "    for idx, ax in enumerate(axes.flatten()):\n",
        "        ax.axis('off')\n",
        "        ax.imshow(figs[idx])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4siFJUe92YiT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "import cv2\n",
        "\n",
        "def plot(samples):\n",
        "    fig = plt.figure(figsize=(8, 8))\n",
        "    gs = gridspec.GridSpec(8, 8)\n",
        "    gs.update(wspace=0.05, hspace=0.05)\n",
        "    \n",
        "    #samples = samples*127.5 + 127.5\n",
        "    \n",
        "    for i, sample in enumerate(samples):\n",
        "        ax = plt.subplot(gs[i])\n",
        "        plt.axis('off')\n",
        "        ax.set_xticklabels([])\n",
        "        ax.set_yticklabels([])\n",
        "        ax.set_aspect('equal')\n",
        "        plt.imshow(sample)\n",
        "\n",
        "    return fig\n",
        "\n",
        "def im_resize(img, des_side):\n",
        "    # get height and width of image\n",
        "    height, width = img.shape[:2]\n",
        "    \n",
        "    #For rgb to grayscale\n",
        "    #img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    \n",
        "    #sqaure image with side equal to height of rectangle\n",
        "    img_sq = cv2.resize(img,(des_side,des_side),interpolation = cv2.INTER_CUBIC)\n",
        "    return img_sq"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HRyfhxMSV5mj",
        "colab_type": "raw"
      },
      "cell_type": "markdown",
      "source": [
        "Train Model"
      ]
    },
    {
      "metadata": {
        "id": "j0UdcrHVV5mk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def save_model_on_imporvemnet(file_path, sess, cv_acc, cv_accs):\n",
        "  #  # save model when there is improvemnet in cv_acc value\n",
        "    if cv_accs == [] or cv_acc > np.max(cv_accs):\n",
        "        saver = tf.train.Saver(max_to_keep = 1)\n",
        "        saver.save(sess, file_path)\n",
        "        print('Model saved')\n",
        "    print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpjkVGTcV5mx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def get_labled_mask(labeled_rate, batch_size):\n",
        "    # get labeled mask to mask some data unlabeled\n",
        "    labeled_mask = np.zeros([batch_size], dtype = np.float32)\n",
        "    labeled_count = np.int(batch_size * labeled_rate)\n",
        "    labeled_mask[range(labeled_count)] = 1.0\n",
        "    #np.random.shuffle(labeled_mask)\n",
        "\n",
        "    return labeled_mask"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hNWJbMDDV5m-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def log_loss_acc(file_path, epoch, train_loss_D, train_loss_G, train_Acc,\n",
        "                 cv_loss_D, cv_loss_G, cv_Acc, log_mode = 'a'):\n",
        "    # log train and cv losses as well as accuracy\n",
        "    mode = log_mode if epoch == 0 else 'a'\n",
        "\n",
        "    with open(file_path, mode) as f:\n",
        "        if mode == 'w':\n",
        "            header = 'epoch, train_loss_D, train_loss_G, train_Acc,' \\\n",
        "                     'cv_loss_D, cv_loss_G, cv_Acc\\n'\n",
        "            f.write(header)\n",
        "\n",
        "        line = '%d, %f, %f, %f, %f, %f, %f\\n' %(epoch, train_loss_D, train_loss_G, train_Acc,\n",
        "                                                cv_loss_D, cv_loss_G, cv_Acc)\n",
        "        f.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UfA2ZCLrV5nE",
        "colab_type": "code",
        "outputId": "dfefd804-7de9-42a7-f96c-ebe06727ea10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 7511
        }
      },
      "cell_type": "code",
      "source": [
        "batch_size = 512\n",
        "prev_epoch = -1\n",
        "epochs = 301\n",
        "\n",
        "\n",
        "#Initialize outputs\n",
        "train_D_losses, train_G_losses, train_Accs = [], [], []\n",
        "cv_D_losses, cv_G_losses, cv_Accs = [], [], []\n",
        "\n",
        "#reset default graph\n",
        "tf.reset_default_graph()\n",
        "\n",
        "#Graph inputs\n",
        "x = tf.placeholder(tf.float32, name = 'x', shape = [None, x_height, x_width, num_channels])\n",
        "label = tf.placeholder(tf.float32, name = 'label', shape = [None, num_classes])\n",
        "labeled_mask = tf.placeholder(tf.float32, name = 'labeled_mask', shape = [None])\n",
        "z = tf.placeholder(tf.float32, name = 'z', shape = [None, latent_size])\n",
        "dropout_rate = tf.placeholder(tf.float32, name = 'dropout_rate')\n",
        "is_training = tf.placeholder(tf.bool, name = 'is_training')\n",
        "G_learning_rate = tf.placeholder(tf.float32, name = 'G_learning_rate')\n",
        "D_learning_rate = tf.placeholder(tf.float32, name = 'D_learning_rate')\n",
        "\n",
        "\n",
        "##Define the model\n",
        "model = build_model(x, z, label, dropout_rate, is_training, print_summary = True)\n",
        "    \n",
        "    \n",
        "D_real_features, D_real_logit, D_real_prob, \\\n",
        "D_fake_features, D_fake_logit, D_fake_prob, \\\n",
        "fake_data = model #\n",
        "\n",
        "#Results\n",
        "extended_label = prepare_labels(label)\n",
        "\n",
        "loss_acc  = loss_accuracy(D_real_features, D_real_logit, D_real_prob,\n",
        "                          D_fake_features, D_fake_logit, D_fake_prob,\n",
        "                              extended_label, labeled_mask)\n",
        "\n",
        "_, _, _, D_L, G_L, accuracy = loss_acc # D_loss, G_loss, accuracy \n",
        "D_optimizer, G_optimizer = optimizer(D_L, G_L, G_learning_rate, D_learning_rate) \n",
        "\n",
        "\n",
        "# Initialize the variables (i.e. assign their default value)\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "# 'Saver' op to save and restore all the variables\n",
        "saver = tf.train.Saver()\n",
        "\n",
        "\n",
        "train_images = dataset['train_images']\n",
        "train_labels = dataset['train_labels']\n",
        "test_images = dataset['test_images']\n",
        "test_labels = dataset['test_labels']\n",
        "\n",
        "\n",
        "# Running first session\n",
        "print(\"Training...\")\n",
        "with tf.Session() as sess:\n",
        "\n",
        "    # Run the initializer\n",
        "    sess.run(init)\n",
        "    \n",
        "    if prev_epoch > 0:\n",
        "      # Restore model weights from previously saved model if any\n",
        "      saver.restore(sess, model_path)\n",
        "      print(\"Model restored at epoch %d from file: %s\" %(prev_epoch, model_path))\n",
        "\n",
        "    for epoch in range(prev_epoch+1, epochs):\n",
        "      t_total = 0\n",
        "      train_size = train_labels.shape[0]\n",
        "      n_iter = int(train_size / batch_size) +  1 \n",
        "      st_ind = 0\n",
        "      train_acc = []\n",
        "      \n",
        "      for iter in range(int(train_images.shape[0] / batch_size) + 1):\n",
        "        t_start = time.time()\n",
        "                \n",
        "        if iter == n_iter - 1:\n",
        "            st_ind = train_size - batch_size - 1\n",
        "            idx = np.random.randint(st_ind, train_size, batch_size)\n",
        "        else:\n",
        "            last_ind = st_ind + batch_size\n",
        "            idx = np.random.randint(st_ind, last_ind, batch_size)\n",
        "            st_ind += batch_size\n",
        "        #print(iter, idx)\n",
        "  \n",
        "        #idx = np.random.randint(0, train_images.shape[0], batch_size)\n",
        "        batch_images = train_images[idx]\n",
        "        batch_labels = train_labels[idx]\n",
        "        batch = (batch_images, batch_labels)\n",
        "                \n",
        "        batch_z = np.random.uniform(-1.0, 1.0, size = (batch_size, latent_size))\n",
        "        mask = get_labled_mask(labeled_rate, batch_size)\n",
        "                \n",
        "        train_feed_dictionary = {x:  normalize(batch[0]),\n",
        "                                         z: batch_z,\n",
        "                                         label: batch[1],\n",
        "                                         labeled_mask: mask,\n",
        "                                         dropout_rate: 0.2,\n",
        "                                         G_learning_rate: 0.0002,\n",
        "                                         D_learning_rate: 0.0002,\n",
        "                                         is_training: True}\n",
        "\n",
        "        D_optimizer.run(feed_dict = train_feed_dictionary)\n",
        "        G_optimizer.run(feed_dict = train_feed_dictionary)\n",
        "\n",
        "        train_D_loss = D_L.eval(feed_dict = train_feed_dictionary)\n",
        "        train_G_loss = G_L.eval(feed_dict = train_feed_dictionary)\n",
        "        train_accuracy = accuracy.eval(feed_dict = train_feed_dictionary)\n",
        "        t_total += (time.time() - t_start)\n",
        "        \n",
        "        train_acc.append(train_accuracy)\n",
        "        \n",
        "      train_accuracy = np.mean(train_acc)          \n",
        "      #pdb.set_trace()\n",
        "      \n",
        "      log_loss_acc(log_path, epoch, train_D_loss, train_G_loss, train_accuracy,\n",
        "                         train_D_loss, train_G_loss, train_accuracy, log_mode = 'w')\n",
        "      print('epoch: ', epoch, 'time: ',t_total, 'train_G_Loss: ',train_G_loss, 'train_D_Loss: ', \n",
        "                  train_D_loss, 'train_acc: ', train_accuracy)\n",
        "      \n",
        "      #print(\"Cross validation!\")\n",
        "      if epoch % 5 == 0:\n",
        "        # Cross-Validation\n",
        "        cv_size = test_labels.shape[0]\n",
        "        n_it = int(cv_size / batch_size) +  1 \n",
        "        st_ind = 0\n",
        "        cv_acc = []\n",
        "      \n",
        "        for test_it in range(n_it):\n",
        "          if test_it == n_it - 1:\n",
        "            st_ind = cv_size - batch_size - 1\n",
        "            idx = np.random.randint(st_ind, cv_size, batch_size)\n",
        "          else:\n",
        "            last_ind = st_ind + batch_size\n",
        "            idx = np.random.randint(st_ind, last_ind, batch_size)\n",
        "            st_ind += batch_size\n",
        "          #print(test_it, idx)\n",
        "  \n",
        "          batch_images = test_images[idx]\n",
        "          batch_labels = test_labels[idx]\n",
        "          test_batch = (batch_images, batch_labels)\n",
        "        \n",
        "          cv_batch_z = np.random.uniform(-1.0, 1.0, size = (batch_size, latent_size))\n",
        "          mask = get_labled_mask(1, batch_size)\n",
        "          cv_feed_dictionary = {x: normalize(test_batch[0]),\n",
        "                                  z: cv_batch_z,\n",
        "                                  label: test_batch[1],\n",
        "                                  labeled_mask: mask,\n",
        "                                  dropout_rate: 0.0,\n",
        "                                  is_training: False}\n",
        "\n",
        "          cv_D_loss = D_L.eval(feed_dict = cv_feed_dictionary)\n",
        "          cv_G_loss = G_L.eval(feed_dict = cv_feed_dictionary)\n",
        "          cv_accuracy = accuracy.eval(feed_dict = cv_feed_dictionary)\n",
        "          cv_acc.append(cv_accuracy)\n",
        "      \n",
        "        cv_accuracy = np.mean(cv_acc)\n",
        "      \n",
        "        \n",
        "        print('\\ncv_G_Loss: %f, cv_D_loss: %f, cv_acc: %f\\n' %(cv_G_loss,\n",
        "                                                                 cv_D_loss,\n",
        "                                                                 cv_accuracy))\n",
        "            \n",
        "        #save_model_on_imporvemnet(model_path, sess, cv_accuracy, cv_Accs)\n",
        "            \n",
        "      \n",
        "        #if cv_Accs == [] or cv_accuracy > np.max(cv_Accs):\n",
        "        save_path = saver.save(sess, model_path)\n",
        "        print(\"Model saved in file: %s\" % save_path)\n",
        "        \n",
        "        fakes = fake_data.eval(feed_dict = train_feed_dictionary)\n",
        "        rescaled_fakes = (fakes + 1.0)/2.0\n",
        "        #plot_fake_data(rescaled_fakes, [8, 8])\n",
        "        fig = plot(rescaled_fakes[:64])\n",
        "        plt.savefig((save_to + 'G@epoch{}.png').format(str(epoch).zfill(3)), bbox_inches = \"tight\")\n",
        "        plt.close()\n",
        "      \n",
        "        cv_D_losses.append(cv_D_loss)\n",
        "        cv_G_losses.append(cv_G_loss)\n",
        "        cv_Accs.append(cv_accuracy)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Discriminator summary:\n",
            " x: (?, 32, 32, 3)\n",
            " D1: (?, 16, 16, 64)\n",
            " D2: (?, 8, 8, 128)\n",
            " D3: (?, 4, 4, 256)\n",
            " D4: (?, 2, 2, 512)\n",
            "\n",
            "(?, 32, 32, 3)\n",
            "Generator summary:\n",
            " z: (?, 2048)\n",
            " G0: (?, 2, 2, 512)\n",
            " G1: (?, 4, 4, 256)\n",
            " G2: (?, 8, 8, 128)\n",
            " G3: (?, 16, 16, 64)\n",
            " G4: (?, 32, 32, 3)\n",
            "\n",
            "Discriminator summary:\n",
            " x: (?, 32, 32, 3)\n",
            " D1: (?, 16, 16, 64)\n",
            " D2: (?, 8, 8, 128)\n",
            " D3: (?, 4, 4, 256)\n",
            " D4: (?, 2, 2, 512)\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-16-51b8259076ef>:8: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "Training...\n",
            "epoch:  0 time:  186.51557111740112 train_G_Loss:  3.7119656 train_D_Loss:  2.0855513 train_acc:  0.20540364\n",
            "\n",
            "cv_G_Loss: 0.249586, cv_D_loss: 7.821461, cv_acc: 0.191598\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  1 time:  181.76082468032837 train_G_Loss:  2.343976 train_D_Loss:  2.1037934 train_acc:  0.32881674\n",
            "epoch:  2 time:  181.5683605670929 train_G_Loss:  3.0249763 train_D_Loss:  1.5392973 train_acc:  0.4498291\n",
            "epoch:  3 time:  181.63065361976624 train_G_Loss:  3.460416 train_D_Loss:  1.4685047 train_acc:  0.58639866\n",
            "epoch:  4 time:  181.8200068473816 train_G_Loss:  3.715546 train_D_Loss:  1.5025536 train_acc:  0.6595323\n",
            "epoch:  5 time:  181.78793811798096 train_G_Loss:  3.5199165 train_D_Loss:  1.1128947 train_acc:  0.7109239\n",
            "\n",
            "cv_G_Loss: 0.521402, cv_D_loss: 3.119225, cv_acc: 0.684321\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  6 time:  181.93342423439026 train_G_Loss:  4.139344 train_D_Loss:  0.77236915 train_acc:  0.73673505\n",
            "epoch:  7 time:  181.83379554748535 train_G_Loss:  4.2041144 train_D_Loss:  0.8741906 train_acc:  0.75968426\n",
            "epoch:  8 time:  180.76151728630066 train_G_Loss:  5.6686487 train_D_Loss:  0.6723154 train_acc:  0.7738173\n",
            "epoch:  9 time:  181.24086904525757 train_G_Loss:  4.234428 train_D_Loss:  0.63394153 train_acc:  0.786594\n",
            "epoch:  10 time:  181.7057545185089 train_G_Loss:  10.425455 train_D_Loss:  0.76238716 train_acc:  0.79710555\n",
            "\n",
            "cv_G_Loss: 2.088936, cv_D_loss: 1.279492, cv_acc: 0.754979\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  11 time:  181.80307340621948 train_G_Loss:  7.807034 train_D_Loss:  0.41257215 train_acc:  0.80578613\n",
            "epoch:  12 time:  180.78935647010803 train_G_Loss:  4.8581867 train_D_Loss:  1.1201338 train_acc:  0.8032498\n",
            "epoch:  13 time:  181.10963439941406 train_G_Loss:  4.173565 train_D_Loss:  0.6786925 train_acc:  0.81340873\n",
            "epoch:  14 time:  181.39220190048218 train_G_Loss:  5.40721 train_D_Loss:  0.4765171 train_acc:  0.8252089\n",
            "epoch:  15 time:  181.2439901828766 train_G_Loss:  5.8376436 train_D_Loss:  0.5076124 train_acc:  0.83048505\n",
            "\n",
            "cv_G_Loss: 0.549087, cv_D_loss: 2.636366, cv_acc: 0.787722\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  16 time:  181.60106372833252 train_G_Loss:  6.24764 train_D_Loss:  0.7298028 train_acc:  0.8330892\n",
            "epoch:  17 time:  181.05639743804932 train_G_Loss:  5.085104 train_D_Loss:  0.56726694 train_acc:  0.8456489\n",
            "epoch:  18 time:  181.05605292320251 train_G_Loss:  4.237004 train_D_Loss:  0.6545579 train_acc:  0.8376058\n",
            "epoch:  19 time:  181.51441550254822 train_G_Loss:  6.3378115 train_D_Loss:  0.56560475 train_acc:  0.8297119\n",
            "epoch:  20 time:  181.94990944862366 train_G_Loss:  5.623118 train_D_Loss:  0.5756216 train_acc:  0.8347575\n",
            "\n",
            "cv_G_Loss: 4.619520, cv_D_loss: 0.976806, cv_acc: 0.806258\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  21 time:  182.19334292411804 train_G_Loss:  4.5355887 train_D_Loss:  0.56184447 train_acc:  0.83877224\n",
            "epoch:  22 time:  181.73753881454468 train_G_Loss:  5.6144023 train_D_Loss:  0.606569 train_acc:  0.8465305\n",
            "epoch:  23 time:  182.0263409614563 train_G_Loss:  5.0850067 train_D_Loss:  0.83232296 train_acc:  0.8412001\n",
            "epoch:  24 time:  182.00986909866333 train_G_Loss:  5.0602927 train_D_Loss:  0.48080865 train_acc:  0.85563153\n",
            "epoch:  25 time:  181.93370127677917 train_G_Loss:  6.5419126 train_D_Loss:  0.6681712 train_acc:  0.8570014\n",
            "\n",
            "cv_G_Loss: 1.020786, cv_D_loss: 1.632507, cv_acc: 0.815296\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  26 time:  181.97451949119568 train_G_Loss:  5.100053 train_D_Loss:  0.69330966 train_acc:  0.85602486\n",
            "epoch:  27 time:  181.78670954704285 train_G_Loss:  6.933033 train_D_Loss:  0.58073235 train_acc:  0.8537055\n",
            "epoch:  28 time:  181.61600422859192 train_G_Loss:  7.3408136 train_D_Loss:  0.35945815 train_acc:  0.85408527\n",
            "epoch:  29 time:  181.62211751937866 train_G_Loss:  3.0007994 train_D_Loss:  0.6933206 train_acc:  0.8602431\n",
            "epoch:  30 time:  181.76265501976013 train_G_Loss:  5.5380015 train_D_Loss:  0.43022454 train_acc:  0.8569336\n",
            "\n",
            "cv_G_Loss: 2.580615, cv_D_loss: 0.955357, cv_acc: 0.834559\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  31 time:  181.95414924621582 train_G_Loss:  4.807782 train_D_Loss:  0.47855502 train_acc:  0.868259\n",
            "epoch:  32 time:  181.67407202720642 train_G_Loss:  6.0593376 train_D_Loss:  0.70715594 train_acc:  0.86503094\n",
            "epoch:  33 time:  181.79681658744812 train_G_Loss:  6.28543 train_D_Loss:  0.617895 train_acc:  0.8659939\n",
            "epoch:  34 time:  181.8567566871643 train_G_Loss:  5.5427213 train_D_Loss:  0.3059539 train_acc:  0.87160915\n",
            "epoch:  35 time:  181.86375761032104 train_G_Loss:  6.3764586 train_D_Loss:  0.50213766 train_acc:  0.87125653\n",
            "\n",
            "cv_G_Loss: 4.458992, cv_D_loss: 0.542370, cv_acc: 0.837737\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  36 time:  182.36779832839966 train_G_Loss:  6.751077 train_D_Loss:  0.30290762 train_acc:  0.86773\n",
            "epoch:  37 time:  182.04085087776184 train_G_Loss:  3.9566023 train_D_Loss:  0.42603505 train_acc:  0.8650038\n",
            "epoch:  38 time:  182.20315384864807 train_G_Loss:  5.6421223 train_D_Loss:  0.48664907 train_acc:  0.8716905\n",
            "epoch:  39 time:  182.23202443122864 train_G_Loss:  6.1866546 train_D_Loss:  0.2342161 train_acc:  0.8705105\n",
            "epoch:  40 time:  182.01796221733093 train_G_Loss:  5.948452 train_D_Loss:  0.328549 train_acc:  0.87282985\n",
            "\n",
            "cv_G_Loss: 2.377411, cv_D_loss: 0.944217, cv_acc: 0.847120\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  41 time:  182.23071718215942 train_G_Loss:  6.4258943 train_D_Loss:  0.37202546 train_acc:  0.87580025\n",
            "epoch:  42 time:  181.9346969127655 train_G_Loss:  5.7357426 train_D_Loss:  0.41819543 train_acc:  0.88172746\n",
            "epoch:  43 time:  181.78643894195557 train_G_Loss:  5.2115993 train_D_Loss:  0.36509553 train_acc:  0.87098527\n",
            "epoch:  44 time:  181.78933119773865 train_G_Loss:  7.226181 train_D_Loss:  0.30345896 train_acc:  0.88190377\n",
            "epoch:  45 time:  181.7554738521576 train_G_Loss:  6.0343127 train_D_Loss:  0.2198286 train_acc:  0.88307023\n",
            "\n",
            "cv_G_Loss: 4.512214, cv_D_loss: 0.526156, cv_acc: 0.847963\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  46 time:  182.16097021102905 train_G_Loss:  4.8800964 train_D_Loss:  0.53819156 train_acc:  0.8808458\n",
            "epoch:  47 time:  182.02406787872314 train_G_Loss:  6.704749 train_D_Loss:  0.43372965 train_acc:  0.8819987\n",
            "epoch:  48 time:  181.8423674106598 train_G_Loss:  6.060703 train_D_Loss:  0.28368428 train_acc:  0.87920463\n",
            "epoch:  49 time:  181.75170755386353 train_G_Loss:  5.276556 train_D_Loss:  0.36867192 train_acc:  0.8840603\n",
            "epoch:  50 time:  181.93999981880188 train_G_Loss:  6.914945 train_D_Loss:  0.42172593 train_acc:  0.88776314\n",
            "\n",
            "cv_G_Loss: 7.652966, cv_D_loss: 0.419119, cv_acc: 0.845473\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  51 time:  182.1233093738556 train_G_Loss:  6.066609 train_D_Loss:  0.44453883 train_acc:  0.8885498\n",
            "epoch:  52 time:  181.99965524673462 train_G_Loss:  6.0136933 train_D_Loss:  0.575109 train_acc:  0.88882107\n",
            "epoch:  53 time:  181.90201330184937 train_G_Loss:  6.907386 train_D_Loss:  0.36564526 train_acc:  0.8890517\n",
            "epoch:  54 time:  181.88368844985962 train_G_Loss:  6.717215 train_D_Loss:  0.20811068 train_acc:  0.89138454\n",
            "epoch:  55 time:  181.77187514305115 train_G_Loss:  5.056369 train_D_Loss:  0.46590394 train_acc:  0.893948\n",
            "\n",
            "cv_G_Loss: 5.759196, cv_D_loss: 0.430188, cv_acc: 0.857958\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  56 time:  182.24223351478577 train_G_Loss:  6.0431247 train_D_Loss:  0.3526997 train_acc:  0.8901096\n",
            "epoch:  57 time:  181.9715530872345 train_G_Loss:  6.2433386 train_D_Loss:  0.32622942 train_acc:  0.8977458\n",
            "epoch:  58 time:  182.03037309646606 train_G_Loss:  7.0434194 train_D_Loss:  0.42212585 train_acc:  0.89279515\n",
            "epoch:  59 time:  182.05157041549683 train_G_Loss:  10.112832 train_D_Loss:  0.3421398 train_acc:  0.8941786\n",
            "epoch:  60 time:  182.10156798362732 train_G_Loss:  10.904786 train_D_Loss:  0.44937634 train_acc:  0.8887126\n",
            "\n",
            "cv_G_Loss: 7.583112, cv_D_loss: 0.612699, cv_acc: 0.855086\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  61 time:  182.1992688179016 train_G_Loss:  10.8626375 train_D_Loss:  0.45233434 train_acc:  0.8929986\n",
            "epoch:  62 time:  182.10076928138733 train_G_Loss:  10.668394 train_D_Loss:  0.39030838 train_acc:  0.89207625\n",
            "epoch:  63 time:  181.87151312828064 train_G_Loss:  5.781666 train_D_Loss:  0.34363508 train_acc:  0.88776314\n",
            "epoch:  64 time:  181.94698929786682 train_G_Loss:  7.9976397 train_D_Loss:  0.44870463 train_acc:  0.8862169\n",
            "epoch:  65 time:  181.6826434135437 train_G_Loss:  4.2765226 train_D_Loss:  0.20315689 train_acc:  0.8980577\n",
            "\n",
            "cv_G_Loss: 3.155997, cv_D_loss: 0.543657, cv_acc: 0.860907\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  66 time:  181.9766309261322 train_G_Loss:  5.933068 train_D_Loss:  0.4277219 train_acc:  0.8945855\n",
            "epoch:  67 time:  181.6454451084137 train_G_Loss:  5.9604907 train_D_Loss:  0.42017025 train_acc:  0.89378524\n",
            "epoch:  68 time:  181.923269033432 train_G_Loss:  10.702698 train_D_Loss:  0.38418323 train_acc:  0.89953613\n",
            "epoch:  69 time:  181.78242826461792 train_G_Loss:  5.326751 train_D_Loss:  0.3984294 train_acc:  0.9027507\n",
            "epoch:  70 time:  181.96614384651184 train_G_Loss:  8.342187 train_D_Loss:  0.34192815 train_acc:  0.9010688\n",
            "\n",
            "cv_G_Loss: 5.276100, cv_D_loss: 0.410934, cv_acc: 0.864354\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  71 time:  182.1754174232483 train_G_Loss:  6.8582034 train_D_Loss:  0.2339938 train_acc:  0.8973931\n",
            "epoch:  72 time:  181.65672039985657 train_G_Loss:  13.684022 train_D_Loss:  0.30411232 train_acc:  0.8993598\n",
            "epoch:  73 time:  181.51796746253967 train_G_Loss:  7.017756 train_D_Loss:  0.27497813 train_acc:  0.898112\n",
            "epoch:  74 time:  181.57245230674744 train_G_Loss:  12.412782 train_D_Loss:  0.64612216 train_acc:  0.898763\n",
            "epoch:  75 time:  181.55220460891724 train_G_Loss:  5.4970026 train_D_Loss:  0.30227432 train_acc:  0.892768\n",
            "\n",
            "cv_G_Loss: 2.126619, cv_D_loss: 1.841570, cv_acc: 0.855890\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  76 time:  181.84695672988892 train_G_Loss:  7.738531 train_D_Loss:  0.2698948 train_acc:  0.9021132\n",
            "epoch:  77 time:  181.52656722068787 train_G_Loss:  4.788529 train_D_Loss:  0.46940127 train_acc:  0.90308976\n",
            "epoch:  78 time:  181.53495359420776 train_G_Loss:  6.1035795 train_D_Loss:  0.42094946 train_acc:  0.90375435\n",
            "epoch:  79 time:  181.58044147491455 train_G_Loss:  6.0955243 train_D_Loss:  0.3468453 train_acc:  0.8996175\n",
            "epoch:  80 time:  181.64759826660156 train_G_Loss:  5.658286 train_D_Loss:  0.47164652 train_acc:  0.900038\n",
            "\n",
            "cv_G_Loss: 3.585592, cv_D_loss: 0.717279, cv_acc: 0.864162\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  81 time:  181.7694869041443 train_G_Loss:  5.9137383 train_D_Loss:  0.331565 train_acc:  0.90493435\n",
            "epoch:  82 time:  181.8835892677307 train_G_Loss:  5.7675757 train_D_Loss:  0.37114242 train_acc:  0.9000651\n",
            "epoch:  83 time:  181.78821659088135 train_G_Loss:  6.7380767 train_D_Loss:  0.33383933 train_acc:  0.89853245\n",
            "epoch:  84 time:  181.87752079963684 train_G_Loss:  6.6405497 train_D_Loss:  0.22042733 train_acc:  0.9051378\n",
            "epoch:  85 time:  181.53176975250244 train_G_Loss:  7.656792 train_D_Loss:  0.41080013 train_acc:  0.8979221\n",
            "\n",
            "cv_G_Loss: 4.115541, cv_D_loss: 0.520944, cv_acc: 0.849341\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  86 time:  182.65719962120056 train_G_Loss:  6.6037297 train_D_Loss:  0.44288507 train_acc:  0.90323895\n",
            "epoch:  87 time:  181.61108255386353 train_G_Loss:  8.209797 train_D_Loss:  0.27652776 train_acc:  0.90116376\n",
            "epoch:  88 time:  181.9259650707245 train_G_Loss:  17.118618 train_D_Loss:  0.54631424 train_acc:  0.90912545\n",
            "epoch:  89 time:  181.67487049102783 train_G_Loss:  6.24695 train_D_Loss:  0.45919132 train_acc:  0.897054\n",
            "epoch:  90 time:  181.69036769866943 train_G_Loss:  5.181264 train_D_Loss:  0.57330704 train_acc:  0.8995904\n",
            "\n",
            "cv_G_Loss: 4.300473, cv_D_loss: 0.701300, cv_acc: 0.869447\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  91 time:  181.9373562335968 train_G_Loss:  6.280897 train_D_Loss:  0.5565428 train_acc:  0.9075928\n",
            "epoch:  92 time:  181.70222568511963 train_G_Loss:  6.166018 train_D_Loss:  0.38575798 train_acc:  0.9088677\n",
            "epoch:  93 time:  181.67724919319153 train_G_Loss:  8.342763 train_D_Loss:  0.15303597 train_acc:  0.9080132\n",
            "epoch:  94 time:  181.76955604553223 train_G_Loss:  4.547371 train_D_Loss:  0.40551907 train_acc:  0.90668404\n",
            "epoch:  95 time:  181.82566261291504 train_G_Loss:  4.8628087 train_D_Loss:  0.2783095 train_acc:  0.9059516\n",
            "\n",
            "cv_G_Loss: 1.851307, cv_D_loss: 1.638921, cv_acc: 0.862975\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  96 time:  182.10142016410828 train_G_Loss:  5.8804045 train_D_Loss:  0.3329366 train_acc:  0.90724015\n",
            "epoch:  97 time:  181.98585438728333 train_G_Loss:  6.6362014 train_D_Loss:  0.2573559 train_acc:  0.9048123\n",
            "epoch:  98 time:  182.10617995262146 train_G_Loss:  7.603852 train_D_Loss:  0.4525038 train_acc:  0.9057481\n",
            "epoch:  99 time:  182.05796480178833 train_G_Loss:  7.9806833 train_D_Loss:  0.27057147 train_acc:  0.9083523\n",
            "epoch:  100 time:  181.87537002563477 train_G_Loss:  6.896313 train_D_Loss:  0.36308628 train_acc:  0.9130317\n",
            "\n",
            "cv_G_Loss: 3.395127, cv_D_loss: 0.803793, cv_acc: 0.865924\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  101 time:  181.81846833229065 train_G_Loss:  10.335339 train_D_Loss:  0.4417371 train_acc:  0.90979004\n",
            "epoch:  102 time:  181.51885437965393 train_G_Loss:  5.071427 train_D_Loss:  0.26547435 train_acc:  0.9058024\n",
            "epoch:  103 time:  181.58240389823914 train_G_Loss:  6.1643114 train_D_Loss:  0.30952802 train_acc:  0.9117974\n",
            "epoch:  104 time:  181.8683090209961 train_G_Loss:  6.348765 train_D_Loss:  0.089641996 train_acc:  0.9136827\n",
            "epoch:  105 time:  181.67386150360107 train_G_Loss:  7.812161 train_D_Loss:  0.29891527 train_acc:  0.91334367\n",
            "\n",
            "cv_G_Loss: 2.104382, cv_D_loss: 1.878348, cv_acc: 0.858226\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  106 time:  181.8470902442932 train_G_Loss:  6.5504956 train_D_Loss:  0.26121208 train_acc:  0.9109565\n",
            "epoch:  107 time:  181.57010960578918 train_G_Loss:  6.4890747 train_D_Loss:  0.20730235 train_acc:  0.9148763\n",
            "epoch:  108 time:  181.59597396850586 train_G_Loss:  8.66504 train_D_Loss:  0.34831908 train_acc:  0.90968156\n",
            "epoch:  109 time:  181.52249240875244 train_G_Loss:  6.2495613 train_D_Loss:  0.2783547 train_acc:  0.9094374\n",
            "epoch:  110 time:  181.58807110786438 train_G_Loss:  4.330863 train_D_Loss:  0.41526538 train_acc:  0.9118788\n",
            "\n",
            "cv_G_Loss: 2.870778, cv_D_loss: 1.340042, cv_acc: 0.861213\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  111 time:  181.94546604156494 train_G_Loss:  6.017212 train_D_Loss:  0.31391084 train_acc:  0.90985787\n",
            "epoch:  112 time:  181.6263177394867 train_G_Loss:  8.58186 train_D_Loss:  0.3761097 train_acc:  0.9091661\n",
            "epoch:  113 time:  181.7121171951294 train_G_Loss:  6.400716 train_D_Loss:  0.19727936 train_acc:  0.91223145\n",
            "epoch:  114 time:  181.77100610733032 train_G_Loss:  5.52957 train_D_Loss:  0.3211588 train_acc:  0.91471356\n",
            "epoch:  115 time:  181.81848168373108 train_G_Loss:  5.509278 train_D_Loss:  0.30947387 train_acc:  0.91844344\n",
            "\n",
            "cv_G_Loss: 3.974041, cv_D_loss: 0.616128, cv_acc: 0.863051\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  116 time:  181.9138286113739 train_G_Loss:  6.300712 train_D_Loss:  0.3039454 train_acc:  0.91707355\n",
            "epoch:  117 time:  181.6573450565338 train_G_Loss:  7.1418686 train_D_Loss:  0.18199424 train_acc:  0.91762966\n",
            "epoch:  118 time:  181.91586899757385 train_G_Loss:  7.5806346 train_D_Loss:  0.41819322 train_acc:  0.9126926\n",
            "epoch:  119 time:  181.9351692199707 train_G_Loss:  5.900584 train_D_Loss:  0.25411046 train_acc:  0.91240776\n",
            "epoch:  120 time:  181.7155282497406 train_G_Loss:  6.9874754 train_D_Loss:  0.24066837 train_acc:  0.91604275\n",
            "\n",
            "cv_G_Loss: 0.932034, cv_D_loss: 2.376801, cv_acc: 0.859681\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  121 time:  181.96233296394348 train_G_Loss:  4.7900734 train_D_Loss:  0.25442627 train_acc:  0.9158122\n",
            "epoch:  122 time:  181.63310980796814 train_G_Loss:  5.5783052 train_D_Loss:  0.36079004 train_acc:  0.918769\n",
            "epoch:  123 time:  181.7162082195282 train_G_Loss:  6.088166 train_D_Loss:  0.09094241 train_acc:  0.91933864\n",
            "epoch:  124 time:  181.72949409484863 train_G_Loss:  8.207866 train_D_Loss:  0.20519853 train_acc:  0.9171821\n",
            "epoch:  125 time:  181.53992700576782 train_G_Loss:  4.6507936 train_D_Loss:  0.15119672 train_acc:  0.915663\n",
            "\n",
            "cv_G_Loss: 1.736137, cv_D_loss: 1.405174, cv_acc: 0.864162\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  126 time:  181.93521356582642 train_G_Loss:  9.566526 train_D_Loss:  0.20696725 train_acc:  0.91786027\n",
            "epoch:  127 time:  181.6559703350067 train_G_Loss:  5.83124 train_D_Loss:  0.17732365 train_acc:  0.9170058\n",
            "epoch:  128 time:  181.58053255081177 train_G_Loss:  5.5998826 train_D_Loss:  0.35182008 train_acc:  0.91799587\n",
            "epoch:  129 time:  181.6709303855896 train_G_Loss:  5.5496206 train_D_Loss:  0.14518785 train_acc:  0.91901314\n",
            "epoch:  130 time:  181.736093044281 train_G_Loss:  5.545201 train_D_Loss:  0.2531495 train_acc:  0.9185113\n",
            "\n",
            "cv_G_Loss: 4.319698, cv_D_loss: 0.610164, cv_acc: 0.868298\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  131 time:  181.8611617088318 train_G_Loss:  6.895256 train_D_Loss:  0.16960888 train_acc:  0.9215495\n",
            "epoch:  132 time:  180.28426146507263 train_G_Loss:  6.146114 train_D_Loss:  0.35160443 train_acc:  0.9211968\n",
            "epoch:  133 time:  178.54881882667542 train_G_Loss:  6.4015613 train_D_Loss:  0.16488402 train_acc:  0.919637\n",
            "epoch:  134 time:  178.7762701511383 train_G_Loss:  7.3980417 train_D_Loss:  0.19719756 train_acc:  0.9180908\n",
            "epoch:  135 time:  178.74706625938416 train_G_Loss:  7.7230735 train_D_Loss:  0.34447315 train_acc:  0.9172092\n",
            "\n",
            "cv_G_Loss: 1.295374, cv_D_loss: 2.339968, cv_acc: 0.869447\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  136 time:  178.75466299057007 train_G_Loss:  7.3958683 train_D_Loss:  0.15326512 train_acc:  0.92108834\n",
            "epoch:  137 time:  178.64736485481262 train_G_Loss:  9.194524 train_D_Loss:  0.16919823 train_acc:  0.9201389\n",
            "epoch:  138 time:  178.5531141757965 train_G_Loss:  11.184344 train_D_Loss:  0.13879973 train_acc:  0.9216851\n",
            "epoch:  139 time:  178.62566089630127 train_G_Loss:  5.5213675 train_D_Loss:  0.3410925 train_acc:  0.91627336\n",
            "epoch:  140 time:  178.70190834999084 train_G_Loss:  5.6340995 train_D_Loss:  0.28172684 train_acc:  0.9232449\n",
            "\n",
            "cv_G_Loss: 1.526114, cv_D_loss: 2.226190, cv_acc: 0.867417\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  141 time:  178.76004028320312 train_G_Loss:  11.505914 train_D_Loss:  0.20403771 train_acc:  0.9208035\n",
            "epoch:  142 time:  178.76581025123596 train_G_Loss:  5.8740606 train_D_Loss:  0.1840431 train_acc:  0.9211697\n",
            "epoch:  143 time:  178.72345733642578 train_G_Loss:  7.112384 train_D_Loss:  0.23515707 train_acc:  0.92458767\n",
            "epoch:  144 time:  178.67289924621582 train_G_Loss:  5.2370534 train_D_Loss:  0.30142358 train_acc:  0.92207843\n",
            "epoch:  145 time:  178.73001718521118 train_G_Loss:  6.5882077 train_D_Loss:  0.26720202 train_acc:  0.9248183\n",
            "\n",
            "cv_G_Loss: 4.063247, cv_D_loss: 0.605266, cv_acc: 0.861596\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  146 time:  178.6576063632965 train_G_Loss:  5.614101 train_D_Loss:  0.21549116 train_acc:  0.92841256\n",
            "epoch:  147 time:  178.71283149719238 train_G_Loss:  6.183843 train_D_Loss:  0.22739916 train_acc:  0.9284261\n",
            "epoch:  148 time:  178.60200214385986 train_G_Loss:  6.2951856 train_D_Loss:  0.14993927 train_acc:  0.92503524\n",
            "epoch:  149 time:  178.6512405872345 train_G_Loss:  7.221543 train_D_Loss:  0.13940519 train_acc:  0.92648655\n",
            "epoch:  150 time:  178.66046261787415 train_G_Loss:  12.813851 train_D_Loss:  0.37229952 train_acc:  0.9234619\n",
            "\n",
            "cv_G_Loss: 10.182559, cv_D_loss: 0.837478, cv_acc: 0.855124\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  151 time:  179.0835485458374 train_G_Loss:  8.103811 train_D_Loss:  0.23554827 train_acc:  0.92561847\n",
            "epoch:  152 time:  179.24131393432617 train_G_Loss:  4.895164 train_D_Loss:  0.25448886 train_acc:  0.9284532\n",
            "epoch:  153 time:  179.25003790855408 train_G_Loss:  6.6851597 train_D_Loss:  0.279834 train_acc:  0.9250081\n",
            "epoch:  154 time:  179.1522467136383 train_G_Loss:  5.106215 train_D_Loss:  0.12559779 train_acc:  0.9238417\n",
            "epoch:  155 time:  179.13526391983032 train_G_Loss:  6.516968 train_D_Loss:  0.33145356 train_acc:  0.93221027\n",
            "\n",
            "cv_G_Loss: 4.479127, cv_D_loss: 0.504484, cv_acc: 0.878676\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  156 time:  179.2568118572235 train_G_Loss:  12.146475 train_D_Loss:  0.26889443 train_acc:  0.9291585\n",
            "epoch:  157 time:  179.42663288116455 train_G_Loss:  6.110725 train_D_Loss:  0.18224508 train_acc:  0.9272732\n",
            "epoch:  158 time:  179.10167789459229 train_G_Loss:  11.2600565 train_D_Loss:  0.166819 train_acc:  0.92144096\n",
            "epoch:  159 time:  179.23498272895813 train_G_Loss:  7.2580523 train_D_Loss:  0.10479895 train_acc:  0.9273953\n",
            "epoch:  160 time:  179.31058382987976 train_G_Loss:  7.176639 train_D_Loss:  0.32541698 train_acc:  0.9337972\n",
            "\n",
            "cv_G_Loss: 2.097777, cv_D_loss: 1.537811, cv_acc: 0.869907\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  161 time:  179.2178921699524 train_G_Loss:  8.245748 train_D_Loss:  0.103087574 train_acc:  0.9267442\n",
            "epoch:  162 time:  179.44489455223083 train_G_Loss:  7.4907594 train_D_Loss:  0.20048259 train_acc:  0.92854816\n",
            "epoch:  163 time:  179.08843541145325 train_G_Loss:  6.053008 train_D_Loss:  0.19127253 train_acc:  0.92804635\n",
            "epoch:  164 time:  179.183331489563 train_G_Loss:  9.5686455 train_D_Loss:  0.06950004 train_acc:  0.92336696\n",
            "epoch:  165 time:  178.96784853935242 train_G_Loss:  6.8178596 train_D_Loss:  0.17536691 train_acc:  0.9300673\n",
            "\n",
            "cv_G_Loss: 3.243596, cv_D_loss: 0.904825, cv_acc: 0.874655\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  166 time:  179.13196206092834 train_G_Loss:  10.716375 train_D_Loss:  0.2538567 train_acc:  0.9323188\n",
            "epoch:  167 time:  179.3255968093872 train_G_Loss:  6.979797 train_D_Loss:  0.16026752 train_acc:  0.9339871\n",
            "epoch:  168 time:  179.1609809398651 train_G_Loss:  8.566112 train_D_Loss:  0.21701318 train_acc:  0.933336\n",
            "epoch:  169 time:  179.23191237449646 train_G_Loss:  5.205996 train_D_Loss:  0.56538224 train_acc:  0.9277615\n",
            "epoch:  170 time:  179.14199423789978 train_G_Loss:  6.46135 train_D_Loss:  0.111275114 train_acc:  0.926039\n",
            "\n",
            "cv_G_Loss: 3.095950, cv_D_loss: 1.038203, cv_acc: 0.869332\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  171 time:  179.0063135623932 train_G_Loss:  6.6773567 train_D_Loss:  0.21936719 train_acc:  0.9291992\n",
            "epoch:  172 time:  179.17819237709045 train_G_Loss:  8.232842 train_D_Loss:  0.13946624 train_acc:  0.93393284\n",
            "epoch:  173 time:  178.9425129890442 train_G_Loss:  4.8072743 train_D_Loss:  0.28839296 train_acc:  0.9355876\n",
            "epoch:  174 time:  179.04852962493896 train_G_Loss:  13.247752 train_D_Loss:  0.08548486 train_acc:  0.929796\n",
            "epoch:  175 time:  179.04299235343933 train_G_Loss:  7.4464884 train_D_Loss:  0.2614885 train_acc:  0.9309218\n",
            "\n",
            "cv_G_Loss: 5.205670, cv_D_loss: 0.870202, cv_acc: 0.861520\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  176 time:  179.1638252735138 train_G_Loss:  5.545557 train_D_Loss:  0.22061926 train_acc:  0.93119305\n",
            "epoch:  177 time:  179.1674199104309 train_G_Loss:  9.214814 train_D_Loss:  0.17222452 train_acc:  0.9313558\n",
            "epoch:  178 time:  178.96053266525269 train_G_Loss:  6.9095163 train_D_Loss:  0.16361317 train_acc:  0.93203396\n",
            "epoch:  179 time:  179.2145802974701 train_G_Loss:  11.148311 train_D_Loss:  0.2870927 train_acc:  0.9347195\n",
            "epoch:  180 time:  179.02095937728882 train_G_Loss:  6.9165688 train_D_Loss:  0.21833184 train_acc:  0.9376628\n",
            "\n",
            "cv_G_Loss: 6.673446, cv_D_loss: 0.944292, cv_acc: 0.878140\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  181 time:  178.99697351455688 train_G_Loss:  6.4433 train_D_Loss:  0.12022525 train_acc:  0.93911403\n",
            "epoch:  182 time:  179.10791945457458 train_G_Loss:  6.067241 train_D_Loss:  0.19345924 train_acc:  0.9346788\n",
            "epoch:  183 time:  178.9080445766449 train_G_Loss:  6.7895865 train_D_Loss:  0.18617652 train_acc:  0.9328613\n",
            "epoch:  184 time:  179.01234936714172 train_G_Loss:  8.174536 train_D_Loss:  0.082038075 train_acc:  0.9340956\n",
            "epoch:  185 time:  179.16903424263 train_G_Loss:  8.790823 train_D_Loss:  0.11156836 train_acc:  0.93288845\n",
            "\n",
            "cv_G_Loss: 7.041554, cv_D_loss: 0.651343, cv_acc: 0.884727\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  186 time:  179.0336208343506 train_G_Loss:  5.533884 train_D_Loss:  0.20062116 train_acc:  0.9350179\n",
            "epoch:  187 time:  179.00966334342957 train_G_Loss:  9.321701 train_D_Loss:  0.16421239 train_acc:  0.9319661\n",
            "epoch:  188 time:  179.00183963775635 train_G_Loss:  6.5053186 train_D_Loss:  0.13838296 train_acc:  0.9322917\n",
            "epoch:  189 time:  178.96335172653198 train_G_Loss:  6.073769 train_D_Loss:  0.26919165 train_acc:  0.9342448\n",
            "epoch:  190 time:  179.2009038925171 train_G_Loss:  5.04304 train_D_Loss:  0.1886887 train_acc:  0.93395996\n",
            "\n",
            "cv_G_Loss: 6.507127, cv_D_loss: 0.573254, cv_acc: 0.873889\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  191 time:  179.1354877948761 train_G_Loss:  10.702839 train_D_Loss:  0.09134422 train_acc:  0.9340142\n",
            "epoch:  192 time:  179.0417196750641 train_G_Loss:  5.5990925 train_D_Loss:  0.22649527 train_acc:  0.9334581\n",
            "epoch:  193 time:  179.1288743019104 train_G_Loss:  8.239048 train_D_Loss:  0.2993721 train_acc:  0.93406844\n",
            "epoch:  194 time:  179.0304822921753 train_G_Loss:  7.901645 train_D_Loss:  0.14395188 train_acc:  0.9361301\n",
            "epoch:  195 time:  179.29546999931335 train_G_Loss:  7.049894 train_D_Loss:  0.19480997 train_acc:  0.9374593\n",
            "\n",
            "cv_G_Loss: 2.319888, cv_D_loss: 1.118298, cv_acc: 0.875689\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  196 time:  179.04720282554626 train_G_Loss:  9.54282 train_D_Loss:  0.30824316 train_acc:  0.9315999\n",
            "epoch:  197 time:  178.99890398979187 train_G_Loss:  5.172936 train_D_Loss:  0.2051608 train_acc:  0.9345161\n",
            "epoch:  198 time:  180.25089645385742 train_G_Loss:  7.54599 train_D_Loss:  0.082689665 train_acc:  0.936103\n",
            "epoch:  199 time:  181.280198097229 train_G_Loss:  9.293444 train_D_Loss:  0.22974534 train_acc:  0.9383816\n",
            "epoch:  200 time:  181.6082181930542 train_G_Loss:  5.9931955 train_D_Loss:  0.25988138 train_acc:  0.9368625\n",
            "\n",
            "cv_G_Loss: 4.931579, cv_D_loss: 0.787385, cv_acc: 0.865349\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  201 time:  181.68643426895142 train_G_Loss:  9.459093 train_D_Loss:  0.24047475 train_acc:  0.93306476\n",
            "epoch:  202 time:  181.52095484733582 train_G_Loss:  7.5182323 train_D_Loss:  0.23991857 train_acc:  0.93618435\n",
            "epoch:  203 time:  181.56427216529846 train_G_Loss:  6.6220903 train_D_Loss:  0.06548895 train_acc:  0.93632\n",
            "epoch:  204 time:  181.55415892601013 train_G_Loss:  7.016592 train_D_Loss:  0.3249561 train_acc:  0.9373101\n",
            "epoch:  205 time:  181.5518925189972 train_G_Loss:  6.157759 train_D_Loss:  0.25401565 train_acc:  0.9383545\n",
            "\n",
            "cv_G_Loss: 3.241303, cv_D_loss: 1.020574, cv_acc: 0.873123\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  206 time:  181.66879892349243 train_G_Loss:  6.6013055 train_D_Loss:  0.42251736 train_acc:  0.9350857\n",
            "epoch:  207 time:  181.5817518234253 train_G_Loss:  5.793038 train_D_Loss:  0.16698799 train_acc:  0.9370117\n",
            "epoch:  208 time:  181.5497772693634 train_G_Loss:  5.399972 train_D_Loss:  0.22168319 train_acc:  0.9369981\n",
            "epoch:  209 time:  181.38334369659424 train_G_Loss:  7.0690618 train_D_Loss:  0.21586998 train_acc:  0.9402805\n",
            "epoch:  210 time:  181.50777173042297 train_G_Loss:  6.0906553 train_D_Loss:  0.1704399 train_acc:  0.9380425\n",
            "\n",
            "cv_G_Loss: 1.829237, cv_D_loss: 1.992813, cv_acc: 0.870672\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  211 time:  181.7225399017334 train_G_Loss:  9.617156 train_D_Loss:  0.1571057 train_acc:  0.9403076\n",
            "epoch:  212 time:  181.5295271873474 train_G_Loss:  5.675991 train_D_Loss:  0.17697534 train_acc:  0.9369846\n",
            "epoch:  213 time:  181.59726190567017 train_G_Loss:  4.135066 train_D_Loss:  0.16740927 train_acc:  0.9402669\n",
            "epoch:  214 time:  181.55337524414062 train_G_Loss:  5.8582425 train_D_Loss:  0.15727441 train_acc:  0.9401177\n",
            "epoch:  215 time:  181.6693618297577 train_G_Loss:  8.944872 train_D_Loss:  0.20868218 train_acc:  0.94140625\n",
            "\n",
            "cv_G_Loss: 5.609348, cv_D_loss: 0.624350, cv_acc: 0.873851\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  216 time:  181.8820366859436 train_G_Loss:  6.1789193 train_D_Loss:  0.25571793 train_acc:  0.93915474\n",
            "epoch:  217 time:  181.49110078811646 train_G_Loss:  7.03747 train_D_Loss:  0.19010586 train_acc:  0.9375271\n",
            "epoch:  218 time:  181.6407344341278 train_G_Loss:  9.471869 train_D_Loss:  0.14426808 train_acc:  0.9434001\n",
            "epoch:  219 time:  181.56971836090088 train_G_Loss:  7.406211 train_D_Loss:  0.22665635 train_acc:  0.93812394\n",
            "epoch:  220 time:  181.54221034049988 train_G_Loss:  6.4329495 train_D_Loss:  0.13617602 train_acc:  0.94102645\n",
            "\n",
            "cv_G_Loss: 3.913798, cv_D_loss: 0.685597, cv_acc: 0.881625\n",
            "\n",
            "Model saved in file: gdrive/My Drive/gan/svhn/model_Jan16/gan_svhn_model.ckpt\n",
            "epoch:  221 time:  181.76425099372864 train_G_Loss:  10.491198 train_D_Loss:  0.1845785 train_acc:  0.9421251\n",
            "epoch:  222 time:  181.5330731868744 train_G_Loss:  11.489594 train_D_Loss:  0.08441473 train_acc:  0.942532\n",
            "epoch:  223 time:  181.6834590435028 train_G_Loss:  7.053088 train_D_Loss:  0.17049818 train_acc:  0.9413249\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}